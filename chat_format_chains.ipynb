{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operating system\n",
    "## load dotenv forn api key \n",
    "## import helpful init model from langchain chain model help to load llm\n",
    "## import prompt template to determine how i write my template of query\n",
    "## import llm chain to combine more than two query\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import  LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a068b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a215dd",
   "metadata": {},
   "source": [
    "## chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168e4120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't experience emotions or feelings in the same way humans do. However, I am functioning optimally and ready to assist you with your requests. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "## 1 chat model se simple ek baat ka response\n",
    "chat_model = init_chat_model(model = \"gemini-2.0-flash\",\n",
    "                             model_provider=\"google-genai\",\n",
    "                             temperature =0.3)\n",
    "response = chat_model.invoke(\"how are you\")\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4102441",
   "metadata": {},
   "source": [
    "## template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6074e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the capital of germany\n"
     ]
    }
   ],
   "source": [
    "## 2 choose best template for query\n",
    "template = PromptTemplate(input_variables = [\"topic\"],\n",
    "                          template = \"what is the capital of {topic}\")\n",
    "prompt = template.format(topic = \"germany\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14929523",
   "metadata": {},
   "source": [
    "## chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517ab826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_10684\\3341751961.py:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = chat_model,prompt = template)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_10684\\3341751961.py:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = chain.run(topic = city)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of China is **Beijing**.\n",
      "The capital of Norway is **Oslo**.\n",
      "The capital of England is **London**.\n",
      "The capital of France is **Paris**.\n",
      "The capital of Canada is **Ottawa**.\n",
      "The capital of Australia is **Canberra**.\n",
      "The capital of Russia is **Moscow**.\n",
      "The capital of Japan is **Tokyo**.\n",
      "The capital of Telangana is **Hyderabad**.\n"
     ]
    }
   ],
   "source": [
    "## 3 to use of chains\n",
    "cities = [\"china\",\"norway\",\"england\",\"france\",\"canada\",\"australia\",\"russia\",\"japan\",\"telangana\"]\n",
    "for city in cities:\n",
    "    chain = LLMChain(llm = chat_model,prompt = template)\n",
    "    res = chain.run(topic = city)\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
